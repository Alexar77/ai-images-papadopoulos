# -*- coding: utf-8 -*-
"""ex_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ztFdkAigXATmqhw5vZc2pZB2ezKShvFk

# Προαπαιτούμενα

load libraries
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import copy

"""device availability"""

device = "cuda" if torch.cuda.is_available() else "cpu"
print("device:", device)

"""manual seed για αναπαραγωγιμότητα"""

SEED = 2022059
torch.manual_seed(SEED)
np.random.seed(SEED)

if device == 'cuda':
    torch.cuda.manual_seed_all(SEED)

"""# Άσκηση 1

## Προετοιμασία

φόρτωση και προετοιμασία δεδομένων CIFAR-100
"""

# κανονικοποίηση
mean = (0.5071, 0.4867, 0.4408)
std  = (0.2675, 0.2565, 0.2761)

# μετασχηματισμοί
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean, std),
])

#φόρτωση δεδομένων
trainset = torchvision.datasets.CIFAR100(
    root="./data", train=True, download=True, transform=transform
)
testset  = torchvision.datasets.CIFAR100(
    root="./data", train=False, download=True, transform=transform
)
classes = trainset.classes

# dataloaders
BATCH_SIZE = 128
NUM_WORKERS = 2
trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)
testloader  = torch.utils.data.DataLoader(testset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

"""Οπτικοποίηση δειγμάτων του dataset"""

def denorm(img_tensor):
    return img_tensor * torch.tensor(std).view(3,1,1) + torch.tensor(mean).view(3,1,1)

images, labels = next(iter(trainloader))
fig, axes = plt.subplots(2, 6, figsize=(12,5))
for ax, img, lab in zip(axes.flatten(), images[:12], labels[:12]):
    img_show = denorm(img).clamp(0,1).permute(1,2,0)
    ax.imshow(img_show); ax.set_title(classes[lab.item()], fontsize=9); ax.axis('off')
plt.tight_layout(); plt.show()

"""## Ορισμός αρχιτεκτονικής

Vanilla CNN με 4 συνελικτικά στρώματα για CIFAR-100


  **Αρχιτεκτονική:**
  - Conv Block 1: Conv2d(3→32) + ReLU + MaxPool
  - Conv Block 2: Conv2d(32→64) + ReLU + MaxPool  
  - Conv Block 3: Conv2d(64→128) + ReLU + MaxPool
  - Conv Block 4: Conv2d(128→256) + ReLU + MaxPool
  - Fully Connected: 256*2*2 → 512 → 100
"""

class VanillaCNN(nn.Module):
    def __init__(self, num_classes=100, n1=32, n2=64, n3=128, n4=256, fc_units=512):
        super().__init__()

        # Backbone (conv + pooling only)
        self.conv1 = nn.Conv2d(3,  n1, kernel_size=3, padding=1)
        self.pool1 = nn.MaxPool2d(2, 2)  # 32 -> 16

        self.conv2 = nn.Conv2d(n1, n2, kernel_size=3, padding=1)
        self.pool2 = nn.MaxPool2d(2, 2)  # 16 -> 8

        self.conv3 = nn.Conv2d(n2, n3, kernel_size=3, padding=1)
        self.pool3 = nn.MaxPool2d(2, 2)  # 8 -> 4

        self.conv4 = nn.Conv2d(n3, n4, kernel_size=3, padding=1)
        self.pool4 = nn.MaxPool2d(2, 2)  # 4 -> 2

        # Head
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(n4 * 2 * 2, fc_units)
        self.fc2 = nn.Linear(fc_units, num_classes)

    def forward(self, x):
        x = self.pool1(F.relu(self.conv1(x)))
        x = self.pool2(F.relu(self.conv2(x)))
        x = self.pool3(F.relu(self.conv3(x)))
        x = self.pool4(F.relu(self.conv4(x)))

        x = self.flatten(x)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Δημιουργία μοντέλου και εμφάνιση αρχιτεκτονικής
model_example = VanillaCNN(num_classes=100)
print(model_example)
print(f"\nΣυνολικές παράμετροι: {sum(p.numel() for p in model_example.parameters()):,}")

"""## Συναρτήσεις Εκπαίδευσης και Αξιολόγησης

**εκπαίδευση**

Εκπαιδεύει το μοντέλο και επιστρέφει ιστορικό εκπαίδευσης
"""

def train_model(model, train_loader, test_loader, criterion, optimizer="SGD",
                num_epochs=20, device='cuda', experiment_name="Experiment"):

    model = model.to(device)
    history = {'train_loss': [], 'train_acc': [], 'test_acc': []}

    for epoch in range(num_epochs):
        # training
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})

        train_loss = running_loss / len(train_loader)
        train_acc = 100 * correct / total

        # TEST ACCURACY (every epoch)
        test_acc = evaluate_model(model, test_loader, device)

        # save history
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['test_acc'].append(test_acc)

        print(f"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, "
              f"Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}%")

    print(f"\n{experiment_name} - Final: Train={train_acc:.2f}%, Test={test_acc:.2f}%")
    return history, train_acc, test_acc

"""**αξιολόγηση**
αξιολογεί το μοντέλο στο test set κ επιστρέφει accuracy
"""

def evaluate_model(model, test_loader, device='cuda'):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    accuracy = 100 * correct / total
    return accuracy

"""Οπτικοποίηση - Learning Curve"""

def plot_learning_curves(results, title="Learning Curves"):
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Training Loss
    for name, data in results.items():
        epochs = range(1, len(data['history']['train_loss']) + 1)
        axes[0].plot(epochs, data['history']['train_loss'], label=f'{name}', marker='o', markersize=4)
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title(f'{title} - Training Loss')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Train vs Test Accuracy (Learning Curve)
    colors = plt.cm.tab10.colors
    for i, (name, data) in enumerate(results.items()):
        epochs = range(1, len(data['history']['train_acc']) + 1)
        axes[1].plot(epochs, data['history']['train_acc'],
                     label=f'{name} (Train)', color=colors[i], linestyle='-', marker='o', markersize=4)
        axes[1].plot(epochs, data['history']['test_acc'],
                     label=f'{name} (Test)', color=colors[i], linestyle='--', marker='s', markersize=4)

    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy (%)')
    axes[1].set_title(f'{title} - Train vs Test Accuracy')
    axes[1].legend(loc='lower right')
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

"""# Πειράματα

## Πείραμα Α: Συγκριτική Μελέτη Συναρτήσεων Κόστους

Συγκρίνουμε:
1. **CrossEntropyLoss**
2. **NLLLoss**

Σταθερές παράμετροι: SGD optimizer, lr=0.01, epochs=20
"""

NUM_EPOCHS = 20
LR = 0.01

results_loss_functions = {}

print("="*60)
print("Πείραμα 1A.1: CrossEntropyLoss")
print("="*60)

model_ce = VanillaCNN(num_classes=100).to(device)
criterion_ce = nn.CrossEntropyLoss()
optimizer_ce = optim.SGD(model_ce.parameters(), lr=LR, momentum=0.9)

history_ce, train_acc_ce, test_acc_ce = train_model(
    model_ce, trainloader, testloader, criterion_ce, optimizer_ce,
    num_epochs=NUM_EPOCHS, device=device, experiment_name="CrossEntropyLoss"
)
results_loss_functions['CrossEntropyLoss'] = {
    'history': history_ce, 'train_acc': train_acc_ce, 'test_acc': test_acc_ce
}

# Πείραμα με NLLLoss (απαιτεί LogSoftmax)
print("="*60)
print("Πείραμα 1A.2: NLLLoss")
print("="*60)

# Τροποποιημένο μοντέλο με LogSoftmax
class VanillaCNN_NLL(VanillaCNN):
    def forward(self, x):
        x = super().forward(x)
        return F.log_softmax(x, dim=1)

model_nll = VanillaCNN_NLL(num_classes=100).to(device)
criterion_nll = nn.NLLLoss()
optimizer_nll = optim.SGD(model_nll.parameters(), lr=LR, momentum=0.9)

history_nll, train_acc_nll, test_acc_nll = train_model(
    model_nll, trainloader, testloader, criterion_nll, optimizer_nll,
    num_epochs=NUM_EPOCHS, device=device, experiment_name="NLLLoss"
)
results_loss_functions['NLLLoss'] = {
    'history': history_nll, 'train_acc': train_acc_nll, 'test_acc': test_acc_nll
}

"""οπτικοποίηση"""

plot_learning_curves(results_loss_functions, title="Πείραμα 1A: Συναρτήσεις Κόστους")

# Πίνακας
print("\n" + "="*60)
print("ΑΠΟΤΕΛΕΣΜΑΤΑ ΠΕΙΡΑΜΑΤΟΣ 1A: ΣΥΝΑΡΤΗΣΕΙΣ ΚΟΣΤΟΥΣ")
print("="*60)
print(f"{'Loss Function':<20} {'Train Acc':>12} {'Test Acc':>12}")
print("-"*44)
for name, data in results_loss_functions.items():
    print(f"{name:<20} {data['train_acc']:>11.2f}% {data['test_acc']:>11.2f}%")

"""## Πείραμα Β: Συγκριτική Μελέτη Μεθόδων Βελτιστοποίησης

Συγκρίνουμε:
1. **SGD** (Stochastic Gradient Descent με momentum)
2. **Adam** (Adaptive Moment Estimation)
3. **AdamW** (Adam με weight decay)

Σταθερές παράμετροι: CrossEntropyLoss, lr=0.001, epochs=10
"""

LR_OPT = 0.001
results_optimizers = {}

optimizers_to_test = {
    'SGD': lambda params: optim.SGD(params, lr=LR_OPT, momentum=0.9),
    'Adam': lambda params: optim.Adam(params, lr=LR_OPT),
    'AdamW': lambda params: optim.AdamW(params, lr=LR_OPT, weight_decay=0.01)
}

criterion = nn.CrossEntropyLoss()

for opt_name, opt_fn in optimizers_to_test.items():
    print("="*60)
    print(f"Πείραμα 1B: {opt_name}")
    print("="*60)

    model = VanillaCNN(num_classes=100).to(device)
    optimizer = opt_fn(model.parameters())


    history, train_acc, test_acc = train_model(
        model, trainloader, testloader, criterion, optimizer,
        num_epochs=NUM_EPOCHS, device=device, experiment_name=opt_name
    )

    results_optimizers[opt_name] = {
        'history': history, 'train_acc': train_acc, 'test_acc': test_acc
    }

"""οπτικοποίηση"""

plot_learning_curves(results_optimizers, title="Πείραμα 1B: Optimizers")

# Πίνακας
print("\n" + "="*60)
print("ΑΠΟΤΕΛΕΣΜΑΤΑ ΠΕΙΡΑΜΑΤΟΣ 1B: OPTIMIZERS")
print("="*60)
print(f"{'Optimizer':<15} {'Train Acc':>12} {'Test Acc':>12}")
print("-"*39)
for name, data in results_optimizers.items():
    print(f"{name:<15} {data['train_acc']:>11.2f}% {data['test_acc']:>11.2f}%")

"""## Πείραμα Γ: Συγκριτική Μελέτη Learning Rate

Δοκιμάζουμε: **0.1, 0.01, 0.001**

Σταθερές παράμετροι: CrossEntropyLoss, SGD, epochs
"""

learning_rates = [0.1, 0.01, 0.001]
results_lr = {}
criterion = nn.CrossEntropyLoss()

for lr in learning_rates:
    print("="*60)
    print(f"Πείραμα 1C: Learning Rate = {lr}")
    print("="*60)

    model = VanillaCNN(num_classes=100).to(device)
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)

    history, train_acc, test_acc = train_model(
        model, trainloader, testloader, criterion, optimizer,
        num_epochs=NUM_EPOCHS, device=device, experiment_name=f"LR={lr}"
    )
    results_lr[f'LR={lr}'] = {
        'history': history, 'train_acc': train_acc, 'test_acc': test_acc
    }

"""Οπτικοποίηση"""

plot_learning_curves(results_lr, title="Πείραμα 1C: Learning Rate")

# Πίνακας
print("\n" + "="*60)
print("ΑΠΟΤΕΛΕΣΜΑΤΑ ΠΕΙΡΑΜΑΤΟΣ 1C: LEARNING RATE")
print("="*60)
print(f"{'Learning Rate':<15} {'Train Acc':>12} {'Test Acc':>12}")
print("-"*39)
for name, data in results_lr.items():
    print(f"{name:<15} {data['train_acc']:>11.2f}% {data['test_acc']:>11.2f}%")

"""# Συνοπτικά Αποτελέσματα"""

print("\n" + "="*70)
print("ΣΥΝΟΠΤΙΚΑ ΑΠΟΤΕΛΕΣΜΑΤΑ ΑΣΚΗΣΗΣ 1: VANILLA CNN ΓΙΑ CIFAR-100")
print("="*70)

print("\n--- Α. Συναρτήσεις Κόστους (SGD, lr=0.01) ---")
for name, data in results_loss_functions.items():
    print(f"  {name}: Train={data['train_acc']:.2f}%, Test={data['test_acc']:.2f}%")

print("\n--- Β. Optimizers (CrossEntropy, lr=0.001) ---")
for name, data in results_optimizers.items():
    print(f"  {name}: Train={data['train_acc']:.2f}%, Test={data['test_acc']:.2f}%")

print("\n--- Γ. Learning Rate (CrossEntropy, SGD) ---")
for name, data in results_lr.items():
    print(f"  {name}: Train={data['train_acc']:.2f}%, Test={data['test_acc']:.2f}%")

print("\n" + "="*70)

